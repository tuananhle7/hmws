{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb34cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from cmws import util\n",
    "import torch\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    DirectionalLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    HardPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    "    BlendParams,\n",
    "    softmax_rgb_blend\n",
    ")\n",
    "from pytorch3d.structures.meshes import (\n",
    "    Meshes,\n",
    "    join_meshes_as_batch,\n",
    "    join_meshes_as_scene,\n",
    ")\n",
    "import numpy as np\n",
    "from cmws.examples.scene_understanding import data, render\n",
    "from cmws import util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bc75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unravel_index(2, (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce6b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = torch.tensor([[[0,1,0],[1,2,0],[0,0,3]]], device=device)\n",
    "num_rows, num_cols = num_blocks.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8d1609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om/user/katiemc/continuous_mws/cmws/util.py:324: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return int(torch.tensor(shape).prod().long().item())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = util.get_num_elements(num_blocks[:-2])\n",
    "\n",
    "num_blocks.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1a0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "nblocks = num_blocks.view(num_samples, num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3b3259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.get_num_elements(num_blocks[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2295417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katiemc/.conda/envs/cmws/lib/python3.7/site-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/katiemc/.conda/envs/cmws/lib/python3.7/site-packages/ipykernel_launcher.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "num_primitives = 3\n",
    "primitives = [\n",
    "    render.Cube(\n",
    "        \"A\", torch.tensor([1.0, 0.0, 0.0], device=device), torch.tensor(0.3, device=device)\n",
    "    ),\n",
    "    render.Cube(\n",
    "        \"B\", torch.tensor([0.0, 1.0, 0.0], device=device), torch.tensor(0.4, device=device)\n",
    "    ),\n",
    "    render.Cube(\n",
    "        \"C\", torch.tensor([0.0, 0.0, 1.0], device=device), torch.tensor(0.5, device=device)\n",
    "    ),\n",
    "][:num_primitives]\n",
    "\n",
    "num_grid_rows=2\n",
    "num_grid_cols = 2\n",
    "stacking_program = []\n",
    "raw_locations = [] \n",
    "num_blocks = [] \n",
    "for row in range(num_grid_rows): \n",
    "    row_sp = [] \n",
    "    row_raw_locs = [] \n",
    "    row_num_blocks=[]\n",
    "    for col in range(num_grid_cols): \n",
    "        # Sample\n",
    "        single_sp = data.sample_stacking_program(\n",
    "            num_primitives, device, fixed_num_blocks=False\n",
    "        )\n",
    "        single_raw_locs = data.sample_raw_locations(single_sp)\n",
    "        single_num_blocks = torch.tensor(len(single_sp), device=device).long()\n",
    "        row_sp.append(single_sp.numpy())\n",
    "        row_raw_locs.append(single_raw_locs.numpy())\n",
    "        row_num_blocks.append(single_num_blocks.numpy())\n",
    "    stacking_program.append(np.array(row_sp))\n",
    "    raw_locations.append(np.array(row_raw_locs))\n",
    "    num_blocks.append(np.array(row_num_blocks))\n",
    "#     stacking_program.append(row_sp)\n",
    "#     raw_locations.append(row_raw_locs)\n",
    "#     num_blocks.append(row_num_blocks)\n",
    "    \n",
    "#     stacking_program.append(torch.Tensor(row_sp))\n",
    "#     raw_locations.append(torch.Tensor(row_raw_locs))\n",
    "#     num_blocks.append(torch.Tensor(row_num_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec42d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([1, 2, 0]), array([2, 0])], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_program[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4800ccf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 2 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-42e6a781a5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacking_program\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 2 (got 2)"
     ]
    }
   ],
   "source": [
    "sp = torch.Tensor(stacking_program)\n",
    "sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_primitives = 3\n",
    "primitives = [\n",
    "    render.Cube(\n",
    "        \"A\", torch.tensor([1.0, 0.0, 0.0], device=device), torch.tensor(0.3, device=device)\n",
    "    ),\n",
    "    render.Cube(\n",
    "        \"B\", torch.tensor([0.0, 1.0, 0.0], device=device), torch.tensor(0.4, device=device)\n",
    "    ),\n",
    "    render.Cube(\n",
    "        \"C\", torch.tensor([0.0, 0.0, 1.0], device=device), torch.tensor(0.5, device=device)\n",
    "    ),\n",
    "][:num_primitives]\n",
    "# num_primitives = len(primitives)\n",
    "\n",
    "# Sample\n",
    "stacking_program = data.sample_stacking_program(\n",
    "    num_primitives, device, fixed_num_blocks=False\n",
    ")\n",
    "raw_locations = data.sample_raw_locations(stacking_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f35e3587",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-89438843aeaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_locations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "raw_locations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c984bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3951072b7f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert [*shape, max_num_blocks, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlocations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_raw_locations_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacking_program\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om/user/katiemc/continuous_mws/cmws/examples/scene_understanding/render.py\u001b[0m in \u001b[0;36mconvert_raw_locations_batched\u001b[0;34m(raw_locations, stacking_program, primitives)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_locations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0mnum_grid_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_grid_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_locations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;31m# Flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "im_size = 32\n",
    "\n",
    "shape = stacking_program.shape[:-1]\n",
    "max_num_blocks = stacking_program.shape[-1]\n",
    "num_elements = util.get_num_elements(shape)\n",
    "num_channels = 3\n",
    "num_blocks = torch.tensor(len(stacking_program), device=device).long()\n",
    "\n",
    "# [num_primitives]\n",
    "square_size = torch.stack([primitive.size for primitive in primitives])\n",
    "# [num_primitives, 3]\n",
    "square_color = torch.stack([primitive.color for primitive in primitives])\n",
    "\n",
    "# Convert [*shape, max_num_blocks, 3]\n",
    "locations = render.convert_raw_locations_batched(raw_locations, stacking_program, primitives)\n",
    "\n",
    "# Flatten\n",
    "num_blocks_flattened = num_blocks.reshape(num_elements)\n",
    "stacking_program_flattened = stacking_program.reshape((num_elements, max_num_blocks))\n",
    "locations_flattened = locations.view((num_elements, max_num_blocks, 3))\n",
    "\n",
    "imgs = render.render_cubes(num_blocks_flattened, square_size[stacking_program_flattened], square_color[stacking_program_flattened], locations_flattened, im_size)\n",
    "imgs = imgs.permute(0, 3, 1, 2)\n",
    "imgs = imgs.view(*[*shape, num_channels, *imgs.shape[-2:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5808dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cubes = num_blocks_flattened\n",
    "sizes = square_size[stacking_program_flattened]\n",
    "colors = square_color[stacking_program_flattened]\n",
    "positions = locations_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e335fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_cubes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-583719370737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmeshes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cubes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cubes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Combine obj meshes into single mesh from rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# https://github.com/facebookresearch/pytorch3d/issues/15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_cubes' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "im_size = 512\n",
    "\n",
    "R, T = look_at_view_transform(1.7, 0, 180,\n",
    "                              at=((0.0, 0.0, -0.5),))\n",
    "# R, T = look_at_view_transform(3.5, 0, 0,\n",
    "#                               up=((0.0, 0.0, 0.0),),\n",
    "#                               at=((0.0, 0.0, -0.5),))\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T,\n",
    "                                )  # fov=45.0)\n",
    "\n",
    "sigma = 1e-4\n",
    "gamma = 1e-4\n",
    "\n",
    "\n",
    "# Settings for rasterizer (optional blur)\n",
    "# https://github.com/facebookresearch/pytorch3d/blob/1c45ec9770ee3010477272e4cd5387f9ccb8cb51/pytorch3d/renderer/mesh/shader.py\n",
    "# implements eqs from SoftRasterizer paper\n",
    "blend_params = BlendParams(sigma=sigma, gamma=gamma) #,background_color=(0.0, 0.0, 0.0))\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=im_size,  # crisper objects + texture w/ higher resolution\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
    "    faces_per_pixel=1,  # increase at cost of GPU memory,\n",
    "    bin_size=None\n",
    ")\n",
    "\n",
    "# Add light from the front\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, 3.0]])\n",
    "\n",
    "# Compose renderer and shader\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights,\n",
    "        blend_params=blend_params\n",
    "    )\n",
    ")\n",
    "\n",
    "# create one mesh per elmt in batch\n",
    "\n",
    "meshes = []\n",
    "for batch_idx, n_cubes in enumerate(num_cubes):\n",
    "    # Combine obj meshes into single mesh from rendering\n",
    "    # https://github.com/facebookresearch/pytorch3d/issues/15\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    textures = []\n",
    "    vert_offset = 0 # offset by vertices from prior meshes\n",
    "    for i, (position, size,color) in enumerate(zip(positions[batch_idx, :n_cubes, :], sizes[batch_idx, :n_cubes],\n",
    "                                                   colors[batch_idx, :n_cubes, :])):\n",
    "        position = torch.tensor([position[0]+1, position[1], position[2]])\n",
    "        cube_vertices, cube_faces = render.get_cube_mesh(position, size)\n",
    "        # For now, apply same color to each mesh vertex (v \\in V)\n",
    "        texture = torch.ones_like(cube_vertices) * color# [V, 3]\n",
    "        # Offset faces (account for diff indexing, b/c treating as one mesh)\n",
    "        cube_faces = cube_faces + vert_offset\n",
    "        vert_offset = cube_vertices.shape[0]\n",
    "        vertices.append(cube_vertices)\n",
    "        faces.append(cube_faces)\n",
    "        textures.append(texture)\n",
    "\n",
    "    # Concatenate data into single mesh\n",
    "    vertices = torch.cat(vertices)\n",
    "    faces = torch.cat(faces)\n",
    "    textures = torch.cat(textures)[None]  # (1, num_verts, 3)\n",
    "    textures = TexturesVertex(verts_features=textures)\n",
    "    # each elmt of verts array is diff mesh in batch\n",
    "    mesh = Meshes(verts=[vertices], faces=[faces], textures=textures)\n",
    "    meshes.append(mesh)\n",
    "\n",
    "batched_mesh = join_meshes_as_batch(meshes)\n",
    "\n",
    "# Render image\n",
    "img = renderer(batched_mesh)   # (B, H, W, 4)\n",
    "\n",
    "# Remove alpha channel and return (B, im_size, im_size, 3)\n",
    "img = img[:, ..., :3]#.detach().squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbb65f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-470ebf4d71d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#plt.axis('off')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(img[0])\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37d6cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([position[0], position[1], 10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a599d4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.7806e-03, -3.0000e-01,  1.0000e+01])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07abd8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
