{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb34cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from cmws import util\n",
    "import torch\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    DirectionalLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    HardPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    "    BlendParams,\n",
    "    softmax_rgb_blend\n",
    ")\n",
    "from pytorch3d.structures.meshes import (\n",
    "    Meshes,\n",
    "    join_meshes_as_batch,\n",
    "    join_meshes_as_scene,\n",
    ")\n",
    "import numpy as np\n",
    "from cmws.examples.scene_understanding import data, render\n",
    "from cmws import util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98c823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "\n",
    "def sample_stacking_program(num_primitives, device, address_suffix=\"\", fixed_num_blocks=False):\n",
    "    \"\"\"Samples blocks to stack from a set [0, ..., num_primitives - 1]\n",
    "    *without* replacement. The number of blocks is stochastic and\n",
    "    can be < num_primitives.\n",
    "\n",
    "    Args\n",
    "        num_primitives (int)\n",
    "        device\n",
    "        address_suffix\n",
    "\n",
    "    Returns [num_blocks] (where num_blocks is stochastic and between 1 and num_primitives\n",
    "        (inclusive))\n",
    "    \"\"\"\n",
    "\n",
    "    # Init\n",
    "    stacking_program = []\n",
    "    available_primitive_ids = list(range(num_primitives))\n",
    "\n",
    "    if fixed_num_blocks:\n",
    "        num_blocks = num_primitives\n",
    "    else:\n",
    "        # Sample num_blocks uniformly from [1, ..., num_primitives] (inclusive)\n",
    "        raw_num_blocks_logits = torch.ones((num_primitives,), device=device)\n",
    "        raw_num_blocks = pyro.sample(\n",
    "            f\"raw_num_blocks{address_suffix}\",\n",
    "            pyro.distributions.Categorical(logits=raw_num_blocks_logits),\n",
    "        )\n",
    "        num_blocks = raw_num_blocks + 1\n",
    "\n",
    "    # Sample primitive ids\n",
    "    for block_id in range(num_blocks):\n",
    "        # Sample primitive\n",
    "        raw_primitive_id_logits = torch.ones((len(available_primitive_ids),), device=device)\n",
    "        raw_primitive_id = pyro.sample(\n",
    "            f\"raw_primitive_id_{block_id}{address_suffix}\",\n",
    "            pyro.distributions.Categorical(logits=raw_primitive_id_logits),\n",
    "        )\n",
    "        primitive_id = available_primitive_ids.pop(raw_primitive_id)\n",
    "\n",
    "        # Add to the stacking program based on previous action\n",
    "        stacking_program.append(primitive_id)\n",
    "\n",
    "    return torch.tensor(stacking_program, device=device)\n",
    "\n",
    "def sample_raw_locations(stacking_program, address_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Samples the (raw) horizontal location of blocks in the stacking program.\n",
    "    p(raw_locations | stacking_program)\n",
    "\n",
    "    Args\n",
    "        stacking_program [num_blocks]\n",
    "\n",
    "    Returns [num_blocks]\n",
    "    \"\"\"\n",
    "    device = stacking_program[0].device\n",
    "    dist = pyro.distributions.Independent(\n",
    "        pyro.distributions.Normal(torch.zeros((len(stacking_program),), device=device), 1),\n",
    "        reinterpreted_batch_ndims=1,\n",
    "    )\n",
    "    return pyro.sample(f\"raw_locations{address_suffix}\", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7c5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives = [\n",
    "    render.Block(\n",
    "        \"A\",\n",
    "        torch.tensor([1.0, 0.0, 0.0], device=device),\n",
    "        torch.tensor([0.3, 0.3, 0.3], device=device),\n",
    "    ),\n",
    "    render.Block(\n",
    "        \"B\",\n",
    "        torch.tensor([0.0, 1.0, 0.0], device=device),\n",
    "        torch.tensor([0.4, 0.4, 0.4], device=device),\n",
    "    ),\n",
    "    render.Block(\n",
    "        \"C\",\n",
    "        torch.tensor([0.0, 0.0, 1.0], device=device),\n",
    "        torch.tensor([0.5, 0.5, 0.5], device=device),\n",
    "    ),\n",
    "    render.Block(\n",
    "        \"D\",\n",
    "        torch.tensor([1.0, 0.0, 0.0], device=device),\n",
    "        torch.tensor([0.5, 0.3, 0.3], device=device),\n",
    "    ),\n",
    "    render.Block(\n",
    "        \"E\",\n",
    "        torch.tensor([1.0, 0.0, 0.0], device=device),\n",
    "        torch.tensor([0.3, 0.5, 0.3], device=device),\n",
    "    ),\n",
    "    render.Block(\n",
    "        \"F\",\n",
    "        torch.tensor([1.0, 0.0, 0.0], device=device),\n",
    "        torch.tensor([0.3, 0.3, 0.5], device=device),\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_primitives = len(primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee9c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZES:  torch.Size([1, 4, 5, 3])\n",
      "img:  torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b25e1987f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine which cells have stacks\n",
    "import cv2 \n",
    "num_grid_rows = 2\n",
    "num_grid_cols = 2\n",
    "max_num_blocks = 5\n",
    "im_size = 256\n",
    "remove_color = False\n",
    "mode =\"block\"\n",
    "shrink_factor=0.01\n",
    "\n",
    "cells = list(itertools.product(range(num_grid_rows), range(num_grid_cols)))\n",
    "num_cells = num_grid_rows * num_grid_cols\n",
    "num_stacks = random.randint(1, num_cells)\n",
    "cells_with_stack = set(random.sample(cells, num_stacks))\n",
    "\n",
    "# Sample\n",
    "num_blocks = []\n",
    "stacking_program = []\n",
    "raw_locations = []\n",
    "for row in range(num_grid_rows):\n",
    "    for col in range(num_grid_cols):\n",
    "        if (row, col) in cells_with_stack:\n",
    "            num_blocks_ = random.randint(1, max_num_blocks)\n",
    "            stacking_program_ = torch.randint(0, num_primitives, [num_blocks_], device=device)\n",
    "            raw_locations_ = torch.randn(num_blocks_, device=device)\n",
    "            num_blocks.append(num_blocks_)\n",
    "            stacking_program_padded = torch.zeros(max_num_blocks, device=device).long()\n",
    "            stacking_program_padded[: num_blocks[-1]] = stacking_program_\n",
    "            raw_locations_padded = torch.zeros(max_num_blocks, device=device)\n",
    "            raw_locations_padded[: num_blocks[-1]] = raw_locations_\n",
    "            stacking_program.append(stacking_program_padded)\n",
    "            raw_locations.append(raw_locations_padded)\n",
    "        else:\n",
    "            num_blocks.append(0)\n",
    "            stacking_program.append(torch.zeros(max_num_blocks, device=device).long())\n",
    "            raw_locations.append(torch.zeros(max_num_blocks, device=device))\n",
    "num_blocks = torch.tensor(num_blocks, device=device).long().view(num_grid_rows, num_grid_cols)\n",
    "stacking_program = torch.stack(stacking_program).view(\n",
    "    num_grid_rows, num_grid_rows, max_num_blocks\n",
    ")\n",
    "raw_locations = torch.stack(raw_locations).view(num_grid_rows, num_grid_rows, max_num_blocks)\n",
    "\n",
    "# Render\n",
    "img = render.render(\n",
    "    primitives, num_blocks, stacking_program, raw_locations, im_size, remove_color=remove_color, mode=mode,\n",
    "    shrink_factor=shrink_factor\n",
    ")\n",
    "print(\"img: \", img.shape)\n",
    "img = img.permute(1,2,0)\n",
    "plt.imshow(img)\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c67cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c984bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 32\n",
    "\n",
    "shape = stacking_program.shape[:-1]\n",
    "max_num_blocks = stacking_program.shape[-1]\n",
    "num_elements = util.get_num_elements(shape)\n",
    "num_channels = 3\n",
    "num_blocks = torch.tensor(len(stacking_program), device=device).long()\n",
    "\n",
    "# [num_primitives]\n",
    "square_size = torch.stack([primitive.size for primitive in primitives])\n",
    "# [num_primitives, 3]\n",
    "square_color = torch.stack([primitive.color for primitive in primitives])\n",
    "\n",
    "# Convert [*shape, max_num_blocks, 3]\n",
    "locations = render.convert_raw_locations_batched(raw_locations, stacking_program, primitives)\n",
    "\n",
    "# Flatten\n",
    "num_blocks_flattened = num_blocks.reshape(num_elements)\n",
    "stacking_program_flattened = stacking_program.reshape((num_elements, max_num_blocks))\n",
    "locations_flattened = locations.view((num_elements, max_num_blocks, 3))\n",
    "\n",
    "imgs = render.render_cubes(num_blocks_flattened, square_size[stacking_program_flattened], square_color[stacking_program_flattened], locations_flattened, im_size)\n",
    "imgs = imgs.permute(0, 3, 1, 2)\n",
    "imgs = imgs.view(*[*shape, num_channels, *imgs.shape[-2:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cubes = num_blocks_flattened\n",
    "sizes = square_size[stacking_program_flattened]\n",
    "colors = square_color[stacking_program_flattened]\n",
    "positions = locations_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e335fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im_size = 512\n",
    "\n",
    "# Create camera\n",
    "R, T = look_at_view_transform(1.0, 90, 180,\n",
    "                              up=((0.0, -1.0, 0.0),),\n",
    "                              at=((0.0, 1, -0.2),))  # view top to see stacking\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T,\n",
    "                               fov=60.0)\n",
    "\n",
    "# Settings for rasterizer (optional blur)\n",
    "# \n",
    "blend_params = BlendParams( sigma = 1e-4, gamma = 1e-4,background_color=(0.0,0.0,0.0))\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=im_size, # crisper objects + texture w/ higher resolution\n",
    "    blur_radius = np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=2, # increase at cost of GPU memory,\n",
    "    bin_size=0\n",
    ")\n",
    "\n",
    "# Add light from the front\n",
    "lights = PointLights(device=device, location=[[0.0, 3.0, 0.0]]) # top light\n",
    "\n",
    "# Compose renderer and shader\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights,\n",
    "        blend_params=blend_params\n",
    "    )\n",
    ")\n",
    "\n",
    "# create one mesh per elmt in batch\n",
    "\n",
    "meshes = []\n",
    "for batch_idx, n_cubes in enumerate(num_cubes):\n",
    "    # Combine obj meshes into single mesh from rendering\n",
    "    # https://github.com/facebookresearch/pytorch3d/issues/15\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    textures = []\n",
    "    vert_offset = 0 # offset by vertices from prior meshes\n",
    "    for i, (position, size,color) in enumerate(zip(positions[batch_idx, :n_cubes, :], sizes[batch_idx, :n_cubes],\n",
    "                                                   colors[batch_idx, :n_cubes, :])):\n",
    "        cube_vertices, cube_faces = render.get_cube_mesh(position, size)\n",
    "        # For now, apply same color to each mesh vertex (v \\in V)\n",
    "        texture = torch.ones_like(cube_vertices) * color# [V, 3]\n",
    "        # Offset faces (account for diff indexing, b/c treating as one mesh)\n",
    "        cube_faces = cube_faces + vert_offset\n",
    "        vert_offset = cube_vertices.shape[0]\n",
    "        vertices.append(cube_vertices)\n",
    "        faces.append(cube_faces)\n",
    "        textures.append(texture)\n",
    "\n",
    "    # Concatenate data into single mesh\n",
    "    vertices = torch.cat(vertices)\n",
    "    faces = torch.cat(faces)\n",
    "    textures = torch.cat(textures)[None]  # (1, num_verts, 3)\n",
    "    textures = TexturesVertex(verts_features=textures)\n",
    "    # each elmt of verts array is diff mesh in batch\n",
    "    mesh = Meshes(verts=[vertices], faces=[faces], textures=textures)\n",
    "    meshes.append(mesh)\n",
    "\n",
    "batched_mesh = join_meshes_as_batch(meshes)\n",
    "\n",
    "# Render image\n",
    "img = renderer(batched_mesh)   # (B, H, W, 4)\n",
    "\n",
    "# Remove alpha channel and return (B, im_size, im_size, 3)\n",
    "img = img[:, ..., :3]#.detach().squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0])\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0])\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "from math import cos, pi, sin\n",
    "from typing import Iterator, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from pytorch3d.structures.meshes import Meshes\n",
    "\n",
    "\n",
    "# Make an iterator over the adjacent pairs: (-1, 0), (0, 1), ..., (N - 2, N - 1)\n",
    "def _make_pair_range(N: int) -> Iterator[Tuple[int, int]]:\n",
    "    i, j = tee(range(-1, N))\n",
    "    next(j, None)\n",
    "    return zip(i, j)\n",
    "r = 0.10\n",
    "R = 0.7\n",
    "sides = 100\n",
    "rings = 5\n",
    "\n",
    "verts = []\n",
    "for i in range(rings):\n",
    "    # phi ranges from 0 to 2 pi (rings - 1) / rings\n",
    "    phi = 2 * pi * i / rings\n",
    "    for j in range(sides):\n",
    "        # theta ranges from 0 to 2 pi (sides - 1) / sides\n",
    "        theta = 2 * pi * j / sides\n",
    "        x = (R + r * cos(theta)) * cos(phi)\n",
    "        y = (R + r * cos(theta)) * sin(phi)\n",
    "        z = r * sin(theta)\n",
    "        # This vertex has index i * sides + j\n",
    "        verts.append([x, y, z])\n",
    "\n",
    "faces = []\n",
    "for i0, i1 in _make_pair_range(rings):\n",
    "    index0 = (i0 % rings) * sides\n",
    "    index1 = (i1 % rings) * sides\n",
    "    for j0, j1 in _make_pair_range(sides):\n",
    "        index00 = index0 + (j0 % sides)\n",
    "        index01 = index0 + (j1 % sides)\n",
    "        index10 = index1 + (j0 % sides)\n",
    "        index11 = index1 + (j1 % sides)\n",
    "        faces.append([index00, index10, index11])\n",
    "        faces.append([index11, index01, index00])\n",
    "\n",
    "verts_list = [torch.tensor(verts, dtype=torch.float32, device=device)]\n",
    "faces_list = [torch.tensor(faces, dtype=torch.int64, device=device)]\n",
    "\n",
    "textures = torch.ones_like(torch.tensor(verts, dtype=torch.float32, device=device)) * color# [V, 3]\n",
    "textures = textures[None]\n",
    "textures = TexturesVertex(verts_features=textures)\n",
    "# each elmt of verts array is diff mesh in batch\n",
    "mesh = Meshes(verts=verts_list, faces=faces_list, textures=textures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cac445",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = renderer(mesh)   # (B, H, W, 4)\n",
    "# Remove alpha channel and return (B, im_size, im_size, 3)\n",
    "img = img[:, ..., :3]#.detach().squeeze().cpu().numpy()\n",
    "plt.imshow(img[0])\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce155211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an iterator over the adjacent pairs: (-1, 0), (0, 1), ..., (N - 2, N - 1)\n",
    "def _make_pair_range(N: int, start=-1) -> Iterator[Tuple[int, int]]:\n",
    "    i, j = tee(range(start, N))\n",
    "    next(j, None)\n",
    "    return zip(i, j)\n",
    "\n",
    "radius = 0.15\n",
    "height = 0.5\n",
    "closed = True\n",
    "sides = 100\n",
    "rings = 5\n",
    "\n",
    "if not (sides > 0):\n",
    "    raise ValueError(\"sides must be > 0.\")\n",
    "if not (rings > 0):\n",
    "    raise ValueError(\"rings must be > 0.\")\n",
    "device = \"cpu\"#device if device else torch.device(\"cpu\")\n",
    "\n",
    "verts = []\n",
    "for h in range(rings):\n",
    "    z = height * h/(rings-1) - height/2\n",
    "    for i in range(sides):\n",
    "        # theta ranges from 0 to 2 pi (sides - 1) / sides\n",
    "        theta = 2 * pi * i / sides\n",
    "        x = radius * cos(theta)\n",
    "        y = radius * sin(theta)\n",
    "        verts.append([x, y, z])\n",
    "if closed:\n",
    "    # bottom center\n",
    "    verts.append([0, 0, -height/2])\n",
    "    #top center\n",
    "    verts.append([0, 0, height/2])\n",
    "\n",
    "faces = []\n",
    "for i0, i1 in _make_pair_range(sides):\n",
    "    index0 = i0 % sides\n",
    "    index1 = i1 % sides\n",
    "    for j in range(rings-1):\n",
    "        index00 = index0 + (j * sides)\n",
    "        index01 = index0 + ((j+1) *sides)\n",
    "        index10 = index1 + (j * sides)\n",
    "        index11 = index1 + ((j+1) *sides)\n",
    "        faces.append([index00, index10, index11])\n",
    "        faces.append([index11, index01, index00])\n",
    "\n",
    "if closed:\n",
    "    # close bottom and top of cylinder\n",
    "    for i0, i1 in _make_pair_range(sides):\n",
    "        index0 = i0 % sides\n",
    "        index1 = i1 % sides\n",
    "        faces.append([index0, len(verts)-2, index1])\n",
    "        faces.append([index1 + (rings-1)*sides, len(verts)-1, index0 + (rings-1)*sides])\n",
    "\n",
    "verts_list = [torch.tensor(verts, dtype=torch.float32, device=device)]\n",
    "faces_list = [torch.tensor(faces, dtype=torch.int64, device=device)]\n",
    "\n",
    "textures = torch.ones_like(torch.tensor(verts, dtype=torch.float32, device=device)) * color# [V, 3]\n",
    "textures = textures[None]\n",
    "textures = TexturesVertex(verts_features=textures)\n",
    "# each elmt of verts array is diff mesh in batch\n",
    "mesh = Meshes(verts=verts_list, faces=faces_list, textures=textures)\n",
    "\n",
    "img = renderer(mesh)   # (B, H, W, 4)\n",
    "# Remove alpha channel and return (B, im_size, im_size, 3)\n",
    "img = img[:, ..., :3]#.detach().squeeze().cpu().numpy()\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "height=1.0\n",
    "radius=1.0\n",
    "count=[32, 32]\n",
    "\"\"\"\n",
    "Create a mesh of a capsule, or a cylinder with hemispheric ends.\n",
    "Parameters\n",
    "----------\n",
    "height : float\n",
    "Center to center distance of two spheres\n",
    "radius : float\n",
    "Radius of the cylinder and hemispheres\n",
    "count : (2,) int\n",
    "Number of sections on latitude and longitude\n",
    "Returns\n",
    "----------\n",
    "capsule : trimesh.Trimesh\n",
    "Capsule geometry with:\n",
    "- cylinder axis is along Z\n",
    "- one hemisphere is centered at the origin\n",
    "- other hemisphere is centered along the Z axis at height\n",
    "\"\"\"\n",
    "height = float(height)\n",
    "radius = float(radius)\n",
    "count = np.array(count, dtype=np.int64)\n",
    "count += np.mod(count, 2)\n",
    "\n",
    "tol_merge = 1e-5\n",
    "tol_zero = 1e-12\n",
    "\n",
    "# create a theta where there is a double band around the equator\n",
    "# so that we can offset the top and bottom of a sphere to\n",
    "# get a nicely meshed capsule\n",
    "theta = np.linspace(0, np.pi, count[0])\n",
    "center = np.clip(np.arctan(tol_merge / radius),\n",
    "         tol_merge, np.inf)\n",
    "offset = np.array([-center, center]) + (np.pi / 2)\n",
    "theta = np.insert(theta,\n",
    "          int(len(theta) / 2),\n",
    "          offset)\n",
    "\n",
    "capsule = uv_sphere(radius=radius,\n",
    "            count=count,\n",
    "            theta=theta)\n",
    "\n",
    "top = capsule.vertices[:, 2] > tol_zero\n",
    "capsule.vertices[top] += [0, 0, height]\n",
    "capsule.metadata.update({'shape': 'capsule',\n",
    "                 'height': height,\n",
    "                 'radius': radius})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 256\n",
    "\n",
    "# Create camera\n",
    "R, T = look_at_view_transform(1.0, 90, 180,\n",
    "                              up=((0.0, -1.0, 0.0),),\n",
    "                              at=((0.0, 1, -0.2),))  # view top to see stacking\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T,\n",
    "                               fov=60.0)\n",
    "\n",
    "# Settings for rasterizer (optional blur)\n",
    "# \n",
    "blend_params = BlendParams( sigma = 1e-4, gamma = 1e-4)#,background_color=(0.0,0.0,0.0))\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=im_size, # crisper objects + texture w/ higher resolution\n",
    "    blur_radius = np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=2, # increase at cost of GPU memory,\n",
    "    bin_size=0\n",
    ")\n",
    "\n",
    "# Add light from the front\n",
    "lights = PointLights(device=device, location=[[0.0, 3.0, 0.0]]) # top light\n",
    "\n",
    "# Compose renderer and shader\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights,\n",
    "        blend_params=blend_params\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = position\n",
    "size = sizes[batch_idx, :n_cubes][0]\n",
    "\n",
    "# hardcoded face indices\n",
    "faces = torch.tensor(\n",
    "    [\n",
    "        1,\n",
    "        3,\n",
    "        0,\n",
    "        4,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        3,\n",
    "        2,\n",
    "        2,\n",
    "        4,\n",
    "        0,\n",
    "        1,\n",
    "        7,\n",
    "        3,\n",
    "        5,\n",
    "        1,\n",
    "        4,\n",
    "        5,\n",
    "        7,\n",
    "        1,\n",
    "        3,\n",
    "        7,\n",
    "        2,\n",
    "        6,\n",
    "        4,\n",
    "        2,\n",
    "        2,\n",
    "        7,\n",
    "        6,\n",
    "        6,\n",
    "        5,\n",
    "        4,\n",
    "        7,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    dtype=torch.int32,\n",
    "    device=device,\n",
    ").view(-1, 3)\n",
    "\n",
    "\n",
    "# vertices of the cube\n",
    "centered_vertices = (\n",
    "    torch.tensor(\n",
    "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1],\n",
    "        dtype=torch.float,\n",
    "        device=device,\n",
    "    ).view(-1, 3)\n",
    "    - 0.5\n",
    ")\n",
    "size = torch.tensor([0.7, 0.5, 1.0])\n",
    "translation = position.clone()\n",
    "print(\"translation: \", translation)\n",
    "#translation[-1] += size[1] / 2\n",
    "print(\"translation: \", translation, \" size: \", size)\n",
    "vertices = centered_vertices * size + translation[None]\n",
    "cube_vertices = vertices\n",
    "cube_faces = faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes = []\n",
    "for batch_idx, n_cubes in enumerate(num_cubes):\n",
    "    # Combine obj meshes into single mesh from rendering\n",
    "    # https://github.com/facebookresearch/pytorch3d/issues/15\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    textures = []\n",
    "    vert_offset = 0 # offset by vertices from prior meshes\n",
    "    \n",
    "    #cube_vertices, cube_faces = render.get_cube_mesh(position, size)\n",
    "    # For now, apply same color to each mesh vertex (v \\in V)\n",
    "    texture = torch.ones_like(cube_vertices) * color# [V, 3]\n",
    "    # Offset faces (account for diff indexing, b/c treating as one mesh)\n",
    "    cube_faces = cube_faces + vert_offset\n",
    "    vert_offset = cube_vertices.shape[0]\n",
    "    vertices.append(cube_vertices)\n",
    "    faces.append(cube_faces)\n",
    "    textures.append(texture)\n",
    "\n",
    "    # Concatenate data into single mesh\n",
    "    vertices = torch.cat(vertices)\n",
    "    faces = torch.cat(faces)\n",
    "    textures = torch.cat(textures)[None]  # (1, num_verts, 3)\n",
    "    textures = TexturesVertex(verts_features=textures)\n",
    "    # each elmt of verts array is diff mesh in batch\n",
    "    mesh = Meshes(verts=[vertices], faces=[faces], textures=textures)\n",
    "    meshes.append(mesh)\n",
    "\n",
    "batched_mesh = join_meshes_as_batch(meshes)\n",
    "\n",
    "# Render image\n",
    "img = renderer(batched_mesh)   # (B, H, W, 4)\n",
    "\n",
    "# Remove alpha channel and return (B, im_size, im_size, 3)\n",
    "img = img[:, ..., :3]#.detach().squeeze().cpu().numpy()\n",
    "\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff3ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes[batch_idx, :n_cubes][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49d759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
